{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.4-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-2.1.4-py3-none-win_amd64.whl (124.9 MB)\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/124.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/124.9 MB 1.3 MB/s eta 0:01:37\n",
      "   ---------------------------------------- 0.8/124.9 MB 1.3 MB/s eta 0:01:33\n",
      "   ---------------------------------------- 1.3/124.9 MB 1.5 MB/s eta 0:01:25\n",
      "    --------------------------------------- 1.6/124.9 MB 1.4 MB/s eta 0:01:30\n",
      "    --------------------------------------- 1.8/124.9 MB 1.4 MB/s eta 0:01:30\n",
      "    --------------------------------------- 2.1/124.9 MB 1.4 MB/s eta 0:01:28\n",
      "    --------------------------------------- 2.4/124.9 MB 1.4 MB/s eta 0:01:29\n",
      "    --------------------------------------- 2.9/124.9 MB 1.4 MB/s eta 0:01:25\n",
      "   - -------------------------------------- 3.4/124.9 MB 1.5 MB/s eta 0:01:19\n",
      "   - -------------------------------------- 4.2/124.9 MB 1.7 MB/s eta 0:01:11\n",
      "   - -------------------------------------- 4.7/124.9 MB 1.8 MB/s eta 0:01:07\n",
      "   - -------------------------------------- 5.2/124.9 MB 1.9 MB/s eta 0:01:05\n",
      "   - -------------------------------------- 5.8/124.9 MB 1.9 MB/s eta 0:01:02\n",
      "   -- ------------------------------------- 6.3/124.9 MB 1.9 MB/s eta 0:01:01\n",
      "   -- ------------------------------------- 6.8/124.9 MB 2.0 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 7.3/124.9 MB 2.0 MB/s eta 0:00:58\n",
      "   -- ------------------------------------- 7.9/124.9 MB 2.1 MB/s eta 0:00:56\n",
      "   -- ------------------------------------- 8.9/124.9 MB 2.2 MB/s eta 0:00:53\n",
      "   --- ------------------------------------ 10.0/124.9 MB 2.3 MB/s eta 0:00:50\n",
      "   --- ------------------------------------ 10.7/124.9 MB 2.4 MB/s eta 0:00:48\n",
      "   --- ------------------------------------ 11.8/124.9 MB 2.5 MB/s eta 0:00:45\n",
      "   ---- ----------------------------------- 12.8/124.9 MB 2.6 MB/s eta 0:00:43\n",
      "   ---- ----------------------------------- 13.6/124.9 MB 2.7 MB/s eta 0:00:42\n",
      "   ---- ----------------------------------- 14.9/124.9 MB 2.8 MB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 16.0/124.9 MB 2.9 MB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 17.0/124.9 MB 3.0 MB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 17.3/124.9 MB 3.0 MB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 18.4/124.9 MB 3.0 MB/s eta 0:00:36\n",
      "   ------ --------------------------------- 19.7/124.9 MB 3.1 MB/s eta 0:00:35\n",
      "   ------ --------------------------------- 20.4/124.9 MB 3.1 MB/s eta 0:00:34\n",
      "   ------ --------------------------------- 21.8/124.9 MB 3.2 MB/s eta 0:00:33\n",
      "   ------- -------------------------------- 23.1/124.9 MB 3.3 MB/s eta 0:00:31\n",
      "   ------- -------------------------------- 24.4/124.9 MB 3.4 MB/s eta 0:00:30\n",
      "   -------- ------------------------------- 25.7/124.9 MB 3.5 MB/s eta 0:00:29\n",
      "   -------- ------------------------------- 27.0/124.9 MB 3.6 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 28.3/124.9 MB 3.6 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 29.6/124.9 MB 3.7 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 30.7/124.9 MB 3.7 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 31.7/124.9 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 33.0/124.9 MB 3.8 MB/s eta 0:00:25\n",
      "   ---------- ----------------------------- 33.8/124.9 MB 3.9 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 34.9/124.9 MB 3.8 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 35.7/124.9 MB 3.9 MB/s eta 0:00:24\n",
      "   ----------- ---------------------------- 36.2/124.9 MB 3.8 MB/s eta 0:00:24\n",
      "   ------------ --------------------------- 37.5/124.9 MB 3.9 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 38.3/124.9 MB 3.9 MB/s eta 0:00:23\n",
      "   ------------ --------------------------- 39.6/124.9 MB 3.9 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 40.6/124.9 MB 4.0 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 41.4/124.9 MB 4.0 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 42.5/124.9 MB 4.0 MB/s eta 0:00:21\n",
      "   ------------- -------------------------- 43.5/124.9 MB 4.0 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 44.8/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 45.6/124.9 MB 4.1 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 46.1/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 46.7/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 47.7/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 48.5/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 49.0/124.9 MB 4.0 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 50.1/124.9 MB 4.0 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 51.1/124.9 MB 4.0 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 51.4/124.9 MB 4.0 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 51.4/124.9 MB 4.0 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 51.4/124.9 MB 4.0 MB/s eta 0:00:19\n",
      "   ----------------- ---------------------- 54.0/124.9 MB 4.0 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 54.5/124.9 MB 4.0 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 55.3/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 55.6/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 56.1/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 56.1/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 56.1/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 58.5/124.9 MB 3.9 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 59.2/124.9 MB 3.9 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 60.3/124.9 MB 3.9 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 61.6/124.9 MB 3.9 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 62.7/124.9 MB 3.9 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 64.0/124.9 MB 4.0 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 65.0/124.9 MB 4.0 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 66.3/124.9 MB 4.0 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 67.6/124.9 MB 4.0 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 68.7/124.9 MB 4.0 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 70.0/124.9 MB 4.1 MB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 71.3/124.9 MB 4.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 72.4/124.9 MB 4.1 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 73.4/124.9 MB 4.1 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 74.7/124.9 MB 4.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 76.3/124.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 77.1/124.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 78.1/124.9 MB 4.2 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 79.4/124.9 MB 4.2 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 80.7/124.9 MB 4.2 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 82.1/124.9 MB 4.2 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 83.4/124.9 MB 4.3 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 84.7/124.9 MB 4.3 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 86.0/124.9 MB 4.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 87.6/124.9 MB 4.3 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 88.6/124.9 MB 4.4 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 89.4/124.9 MB 4.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 91.0/124.9 MB 4.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 92.3/124.9 MB 4.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 93.6/124.9 MB 4.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 94.4/124.9 MB 4.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 95.7/124.9 MB 4.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 97.0/124.9 MB 4.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 98.0/124.9 MB 4.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 99.4/124.9 MB 4.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 100.7/124.9 MB 4.5 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 102.0/124.9 MB 4.5 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 103.3/124.9 MB 4.5 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 104.3/124.9 MB 4.5 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 105.1/124.9 MB 4.5 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 105.9/124.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 106.7/124.9 MB 4.5 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 107.7/124.9 MB 4.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 108.8/124.9 MB 4.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 110.1/124.9 MB 4.5 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 111.4/124.9 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 112.5/124.9 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 113.8/124.9 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 115.1/124.9 MB 4.6 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 116.4/124.9 MB 4.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 118.0/124.9 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 119.0/124.9 MB 4.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 120.6/124.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  121.9/124.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  122.9/124.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  123.7/124.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  124.8/124.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 124.9/124.9 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./dataset/twitter_training.csv', header=None)\n",
    "val_data = pd.read_csv('./dataset/twitter_validation.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  information      type  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "74681  9200       Nvidia  Positive   \n",
       "\n",
       "                                                    text  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      I am coming to the borders and I will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "74677  Just realized that the Windows partition of my...  \n",
       "74678  Just realized that my Mac window partition is ...  \n",
       "74679  Just realized the windows partition of my Mac ...  \n",
       "74680  Just realized between the windows partition of...  \n",
       "74681  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[74682 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns = ['id', 'information', 'type', 'text']\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          information        type  \\\n",
       "0    3364             Facebook  Irrelevant   \n",
       "1     352               Amazon     Neutral   \n",
       "2    8312            Microsoft    Negative   \n",
       "3    4371                CS-GO    Negative   \n",
       "4    4433               Google     Neutral   \n",
       "..    ...                  ...         ...   \n",
       "995  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "996  4359                CS-GO  Irrelevant   \n",
       "997  2652          Borderlands    Positive   \n",
       "998  8069            Microsoft    Positive   \n",
       "999  6960      johnson&johnson     Neutral   \n",
       "\n",
       "                                                  text  \n",
       "0    I mentioned on Facebook that I was struggling ...  \n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
       "2    @Microsoft Why do I pay for WORD when it funct...  \n",
       "3    CSGO matchmaking is so full of closet hacking,...  \n",
       "4    Now the President is slapping Americans in the...  \n",
       "..                                                 ...  \n",
       "995  ⭐️ Toronto is the arts and culture capital of ...  \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...  \n",
       "997  Today sucked so it’s time to drink wine n play...  \n",
       "998  Bought a fraction of Microsoft today. Small wins.  \n",
       "999  Johnson & Johnson to stop selling talc baby po...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.columns = ['id', 'information', 'type', 'text']\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "      <td>i am coming to the borders and i will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74677</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "      <td>just realized that the windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74678</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "      <td>just realized that my mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74679</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "      <td>just realized the windows partition of my mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74680</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "      <td>just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74681</th>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "      <td>just like the windows partition of my mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74682 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  information      type  \\\n",
       "0      2401  Borderlands  Positive   \n",
       "1      2401  Borderlands  Positive   \n",
       "2      2401  Borderlands  Positive   \n",
       "3      2401  Borderlands  Positive   \n",
       "4      2401  Borderlands  Positive   \n",
       "...     ...          ...       ...   \n",
       "74677  9200       Nvidia  Positive   \n",
       "74678  9200       Nvidia  Positive   \n",
       "74679  9200       Nvidia  Positive   \n",
       "74680  9200       Nvidia  Positive   \n",
       "74681  9200       Nvidia  Positive   \n",
       "\n",
       "                                                    text  \\\n",
       "0      im getting on borderlands and i will murder yo...   \n",
       "1      I am coming to the borders and I will kill you...   \n",
       "2      im getting on borderlands and i will kill you ...   \n",
       "3      im coming on borderlands and i will murder you...   \n",
       "4      im getting on borderlands 2 and i will murder ...   \n",
       "...                                                  ...   \n",
       "74677  Just realized that the Windows partition of my...   \n",
       "74678  Just realized that my Mac window partition is ...   \n",
       "74679  Just realized the windows partition of my Mac ...   \n",
       "74680  Just realized between the windows partition of...   \n",
       "74681  Just like the windows partition of my Mac is l...   \n",
       "\n",
       "                                                   lower  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      i am coming to the borders and i will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "74677  just realized that the windows partition of my...  \n",
       "74678  just realized that my mac window partition is ...  \n",
       "74679  just realized the windows partition of my mac ...  \n",
       "74680  just realized between the windows partition of...  \n",
       "74681  just like the windows partition of my mac is l...  \n",
       "\n",
       "[74682 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['lower'] = train_data['text'].str.lower()\n",
    "train_data['lower'] = [str(data) for data in train_data['lower']]\n",
    "train_data['lower'] = train_data['lower'].apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>information</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling ...</td>\n",
       "      <td>i mentioned on facebook that i was struggling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects clai...</td>\n",
       "      <td>bbc news   amazon boss jeff bezos rejects clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it funct...</td>\n",
       "      <td>microsoft why do i pay for word when it funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking,...</td>\n",
       "      <td>csgo matchmaking is so full of closet hacking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the...</td>\n",
       "      <td>now the president is slapping americans in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>⭐️ Toronto is the arts and culture capital of ...</td>\n",
       "      <td>toronto is the arts and culture capital of c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...</td>\n",
       "      <td>this is actually a good move tot bring more vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Today sucked so it’s time to drink wine n play...</td>\n",
       "      <td>today sucked so it s time to drink wine n play...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "      <td>bought a fraction of microsoft today  small wins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby po...</td>\n",
       "      <td>johnson   johnson to stop selling talc baby po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          information        type  \\\n",
       "0    3364             Facebook  Irrelevant   \n",
       "1     352               Amazon     Neutral   \n",
       "2    8312            Microsoft    Negative   \n",
       "3    4371                CS-GO    Negative   \n",
       "4    4433               Google     Neutral   \n",
       "..    ...                  ...         ...   \n",
       "995  4891  GrandTheftAuto(GTA)  Irrelevant   \n",
       "996  4359                CS-GO  Irrelevant   \n",
       "997  2652          Borderlands    Positive   \n",
       "998  8069            Microsoft    Positive   \n",
       "999  6960      johnson&johnson     Neutral   \n",
       "\n",
       "                                                  text  \\\n",
       "0    I mentioned on Facebook that I was struggling ...   \n",
       "1    BBC News - Amazon boss Jeff Bezos rejects clai...   \n",
       "2    @Microsoft Why do I pay for WORD when it funct...   \n",
       "3    CSGO matchmaking is so full of closet hacking,...   \n",
       "4    Now the President is slapping Americans in the...   \n",
       "..                                                 ...   \n",
       "995  ⭐️ Toronto is the arts and culture capital of ...   \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VI...   \n",
       "997  Today sucked so it’s time to drink wine n play...   \n",
       "998  Bought a fraction of Microsoft today. Small wins.   \n",
       "999  Johnson & Johnson to stop selling talc baby po...   \n",
       "\n",
       "                                                 lower  \n",
       "0    i mentioned on facebook that i was struggling ...  \n",
       "1    bbc news   amazon boss jeff bezos rejects clai...  \n",
       "2     microsoft why do i pay for word when it funct...  \n",
       "3    csgo matchmaking is so full of closet hacking ...  \n",
       "4    now the president is slapping americans in the...  \n",
       "..                                                 ...  \n",
       "995    toronto is the arts and culture capital of c...  \n",
       "996  this is actually a good move tot bring more vi...  \n",
       "997  today sucked so it s time to drink wine n play...  \n",
       "998  bought a fraction of microsoft today  small wins   \n",
       "999  johnson   johnson to stop selling talc baby po...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['lower'] = val_data['text'].str.lower()\n",
    "val_data['lower'] = [str(data) for data in val_data['lower']]\n",
    "val_data['lower'] = val_data['lower'].apply(lambda x: re.sub('[^A-Za-z0-9 ]+', ' ', x))\n",
    "\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokens_text \u001b[38;5;241m=\u001b[39m [word_tokenize(\u001b[38;5;28mstr\u001b[39m(word)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m tokens_counter \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens_text \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m sublist]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(tokens_counter)))\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens_text = [word_tokenize(str(word)) for word in train_data['lower']]\n",
    "tokens_counter = [item for sublist in tokens_text for item in sublist]\n",
    "\n",
    "print(len(set(tokens_counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nltk = nltk.corpus.stopwords\n",
    "stop_words = stopwords_nltk.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_count = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train, reviews_test = train_test_split(train_data, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_bow \u001b[38;5;241m=\u001b[39m bow_count\u001b[38;5;241m.\u001b[39mfit_transform(reviews_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m X_test_bow \u001b[38;5;241m=\u001b[39m bow_count\u001b[38;5;241m.\u001b[39mtransform(reviews_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1259\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1258\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:110\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    108\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS/nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\ASUS\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ASUS\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = bow_count.fit_transform(reviews_train['lower'])\n",
    "X_test_bow = bow_count.transform(reviews_test['lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bow = reviews_train['type']\n",
    "y_test_bow = reviews_test['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "Negative      0.299190\n",
       "Positive      0.282252\n",
       "Neutral       0.245632\n",
       "Irrelevant    0.172926\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_bow.value_counts() / y_test_bow.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LogisticRegression(C=1, solver=\"liblinear\", max_iter=200)\n",
    "model1.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "test_pred = model1.predict(X_test_bow)\n",
    "accuracy_score(y_test_bow, test_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_bow = bow_count.transform(val_data['lower'])\n",
    "y_val_bow = val_data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model1.predict(X_val_bow)\n",
    "accuracy_score(y_val_bow, val_pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_count = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    ngram_range=(1, 4)\n",
    ")\n",
    "\n",
    "X_train_bow = bow_count.fit_transform(reviews_train['lower'])\n",
    "X_test_bow = bow_count.transform(reviews_test['lower'])\n",
    "X_val_bow = bow_count.transform(val_data['lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(C=0-9, solver=\"liblinear\", max_iter=1500)\n",
    "model2.fit(X_train_bow, y_train_bow)\n",
    "test_pred_2 = model2.predict(X_test_bow)\n",
    "accuracy_score(y_test_bow, test_pred_2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_2 = model2.predict(X_val_bow)\n",
    "accuracy_score(y_val_bow, val_pred_2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_bow_num = le.fit_transform(y_train_bow)\n",
    "y_test_bow_num = le.transform(y_test_bow)\n",
    "y_val_bow_num = le.transform(y_val_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(objective=\"multi:softmax\",n_estimators=1000,colsample_bytree=0.6, subsample=0.6)\n",
    "XGB.fit(X_train_bow, y_train_bow_num)\n",
    "\n",
    "test_pred_2 = XGB.predict(X_test_bow)\n",
    "accuracy_score(y_test_bow_num, test_pred_2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred_2 = XGB.predict(X_val_bow)\n",
    "accuracy_score(y_val_bow_num, val_pred_2) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
